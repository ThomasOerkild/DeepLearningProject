{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T20:12:53.468411Z",
     "start_time": "2017-12-10T20:12:50.200924Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import Utils\n",
    "import os\n",
    "import keras\n",
    "from time import gmtime, strftime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T20:12:56.562610Z",
     "start_time": "2017-12-10T20:12:54.689812Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "number_of_classes = 12\n",
    "batch_size = 3\n",
    "input_shape = (batch_size, 352, 480, 3)\n",
    "label_shape = (batch_size, 352, 480, number_of_classes)\n",
    "dropout_prob = 0.5\n",
    "strides = (1,1,1,1)\n",
    "padding = 'SAME'\n",
    "max_pooling_ksize = (1,2,2,1)\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 125\n",
    "\n",
    "baysian_segnet_path = os.path.join('..','saved_models',\"baysian_segnet\",\"baysian_segnet_model.ckpt\")\n",
    "segnet_path = os.path.join('..','saved_models',\"segnet\",\"segnet_model.ckpt\")\n",
    "\n",
    "train_data_path = os.path.join(\"..\",\"Data\", \"CamVid\",\"train\")\n",
    "test_data_path = os.path.join(\"..\",\"Data\", \"CamVid\",\"test\")\n",
    "train_anno_path = os.path.join(\"..\",\"Data\", \"CamVid\",\"trainannot\")\n",
    "test_anno_path = os.path.join(\"..\",\"Data\", \"CamVid\",\"testannot\")\n",
    "\n",
    "class SegNet:\n",
    "    \n",
    "    \n",
    "    def __init__(self, load_model=False, baysian=False):\n",
    "        self.baysian= baysian\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self.build_model()\n",
    "        if load_model:\n",
    "            self.load_model()\n",
    "        else:\n",
    "            self.train_model()\n",
    "            \n",
    "    \n",
    "    def load_model(self):\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        if self.baysian:\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(self.sess, baysian_segnet_path)\n",
    "        else:\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(self.sess, segnet_path)\n",
    "         \n",
    "    \n",
    "    def feed_forward(self, images):\n",
    "        assert np.shape(images) == input_shape\n",
    "        return self.sess.run(self.prediction, feed_dict={self.images: images})\n",
    "        \n",
    "    \n",
    "    def predict(self, images):\n",
    "        assert np.shape(images) == input_shape\n",
    "        \n",
    "        if self.baysian:\n",
    "            predictions = []\n",
    "            for i in range(50):\n",
    "                predictions.append(self.feed_forward(images))\n",
    "            \n",
    "            return np.mean(predictions, axis=0), np.var(predictions, axis=0)\n",
    "            \n",
    "        else:\n",
    "            return self.feed_forward(images)\n",
    "        \n",
    "    def build_model(self):\n",
    "        self.images=tf.placeholder(dtype=tf.float32,shape=input_shape)\n",
    "        self.labels_im=tf.placeholder(dtype=tf.float32,shape=label_shape)\n",
    "        \n",
    "        self.scaling_vector = tf.placeholder(dtype=tf.float32, shape=[number_of_classes])\n",
    "        \n",
    "        vgg16 = keras.applications.VGG16(include_top=False,weights='imagenet')\n",
    "        vgg16_graph = keras.backend.get_session().graph\n",
    "        w_1_1 = vgg16_graph.get_tensor_by_name('block1_conv1/kernel:0')\n",
    "        b_1_1 = vgg16_graph.get_tensor_by_name('block1_conv1/bias:0')\n",
    "        w_1_2 = vgg16_graph.get_tensor_by_name('block1_conv2/kernel:0')\n",
    "        b_1_2 = vgg16_graph.get_tensor_by_name('block1_conv2/bias:0')\n",
    "\n",
    "        w_2_1 = vgg16_graph.get_tensor_by_name('block2_conv1/kernel:0')\n",
    "        b_2_1 = vgg16_graph.get_tensor_by_name('block2_conv1/bias:0')\n",
    "        w_2_2 = vgg16_graph.get_tensor_by_name('block2_conv2/kernel:0')\n",
    "        b_2_2 = vgg16_graph.get_tensor_by_name('block2_conv2/bias:0')\n",
    "\n",
    "        w_3_1 = vgg16_graph.get_tensor_by_name('block3_conv1/kernel:0')\n",
    "        b_3_1 = vgg16_graph.get_tensor_by_name('block3_conv1/bias:0')\n",
    "        w_3_2 = vgg16_graph.get_tensor_by_name('block3_conv2/kernel:0')\n",
    "        b_3_2 = vgg16_graph.get_tensor_by_name('block3_conv2/bias:0')\n",
    "        w_3_3 = vgg16_graph.get_tensor_by_name('block3_conv3/kernel:0')\n",
    "        b_3_3 = vgg16_graph.get_tensor_by_name('block3_conv3/bias:0')\n",
    "\n",
    "        w_4_1 = vgg16_graph.get_tensor_by_name('block4_conv1/kernel:0')\n",
    "        b_4_1 = vgg16_graph.get_tensor_by_name('block4_conv1/bias:0')\n",
    "        w_4_2 = vgg16_graph.get_tensor_by_name('block4_conv2/kernel:0')\n",
    "        b_4_2 = vgg16_graph.get_tensor_by_name('block4_conv2/bias:0')\n",
    "        w_4_3 = vgg16_graph.get_tensor_by_name('block4_conv3/kernel:0')\n",
    "        b_4_3 = vgg16_graph.get_tensor_by_name('block4_conv3/bias:0')\n",
    "\n",
    "        w_5_1 = vgg16_graph.get_tensor_by_name('block5_conv1/kernel:0')\n",
    "        b_5_1 = vgg16_graph.get_tensor_by_name('block5_conv1/bias:0')\n",
    "        w_5_2 = vgg16_graph.get_tensor_by_name('block5_conv2/kernel:0')\n",
    "        b_5_2 = vgg16_graph.get_tensor_by_name('block5_conv2/bias:0')\n",
    "        w_5_3 = vgg16_graph.get_tensor_by_name('block5_conv3/kernel:0')\n",
    "        b_5_3 = vgg16_graph.get_tensor_by_name('block5_conv3/bias:0')\n",
    "        \n",
    "        max_pooling_1, indicies_1 = self.encoding_layer(self.images, w_1_1, w_1_2, b_1_1, b_1_2, conv_stride=strides, name=\"Encoding_layer_1\")\n",
    "\n",
    "        max_pooling_2, indicies_2 = self.encoding_layer(max_pooling_1, w_2_1, w_2_2, b_2_1, b_2_2, conv_stride=strides, name=\"Encoding_layer_2\")\n",
    "        max_pooling_3, indicies_3 = self.encoding_layer(max_pooling_2, w_3_1, w_3_2, b_3_1, b_3_2, conv_stride=strides, name=\"Encoding_layer_3\", filter3=w_3_3, bias3=b_3_3, dropout=self.baysian)\n",
    "        max_pooling_4, indicies_4 = self.encoding_layer(max_pooling_3, w_4_1, w_4_2, b_4_1, b_4_2, conv_stride=strides, name=\"Encoding_layer_4\", filter3=w_4_3, bias3=b_4_3, dropout=self.baysian)\n",
    "        max_pooling_5, indicies_5 = self.encoding_layer(max_pooling_4, w_5_1, w_5_2, b_5_1, b_5_2, conv_stride=strides, name=\"Encoding_layer_5\", filter3=w_5_3, bias3=b_5_3, dropout=self.baysian)\n",
    "\n",
    "        deconv_1 = self.decoding_layer(max_pooling_5, indicies_5, w_5_1.shape[2].value, w_5_2.shape[2].value, name= \"Decoding_layer_1\", filter3_shape=w_5_3.shape[2].value, dropout=self.baysian)\n",
    "        deconv_2 = self.decoding_layer(deconv_1, indicies_4, w_4_1.shape[2].value, w_4_2.shape[2].value, name= \"Decoding_layer_2\", filter3_shape=w_4_3.shape[2].value, dropout=self.baysian)\n",
    "        deconv_3 = self.decoding_layer(deconv_2, indicies_3, w_3_1.shape[2].value, w_3_2.shape[2].value, name= \"Decoding_layer_3\", filter3_shape=w_3_3.shape[2].value, dropout=self.baysian)\n",
    "        deconv_4 = self.decoding_layer(deconv_3, indicies_2, w_2_1.shape[2].value, w_2_2.shape[2].value, name= \"Decoding_layer_4\")\n",
    "        deconv_5 = self.decoding_layer(deconv_4, indicies_1, number_of_classes, w_1_2.shape[2].value, name= \"Decoding_layer_5\")\n",
    "\n",
    "        out = tf.nn.softmax(deconv_5) # (batch_size, width, height, num_classes)\n",
    "        weights = tf.multiply(self.labels_im, self.scaling_vector)\n",
    "        weights = tf.reduce_sum(weights, 3)\n",
    "        self.loss = tf.losses.softmax_cross_entropy(onehot_labels=self.labels_im, logits=deconv_5, weights=weights)\n",
    "        \n",
    "        #self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=scaled_out, labels=scaled_labels))\n",
    "        self.train_step = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "        self.prediction = tf.argmax(out,-1)\n",
    "        correctPred = tf.equal(self.prediction, tf.argmax(self.labels_im,-1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "    \n",
    "    def train_model(self, model_is_loaded=False):\n",
    "        train_accu = tf.summary.scalar(\"train_accu\", self.accuracy)\n",
    "        train_loss = tf.summary.scalar(\"train_loss\", self.loss)\n",
    "        train_summ = tf.summary.merge((train_accu, train_loss))\n",
    "        \n",
    "        if not model_is_loaded:\n",
    "            init = tf.global_variables_initializer()\n",
    "            self.sess = tf.Session()\n",
    "            self.sess.run(init)\n",
    "            \n",
    "        if self.baysian:\n",
    "            logdir = os.path.join(\"tensorboard_logs\",\"Baysian_SegNet\"+strftime(\"_%Y-%m-%d\", gmtime()))\n",
    "        else:\n",
    "            logdir = os.path.join(\"tensorboard_logs\",\"SegNet\"+strftime(\"_%Y-%m-%d\", gmtime()))\n",
    "        writer = tf.summary.FileWriter(logdir)\n",
    "        writer.add_graph(self.sess.graph)\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        train_images = np.asarray(Utils.load_images(train_data_path))\n",
    "        test_images = np.asarray(Utils.load_images(test_data_path))\n",
    "        \n",
    "        train_annotations = np.asarray(Utils.load_annotations(train_anno_path))\n",
    "        test_annotations = np.asarray(Utils.load_annotations(test_anno_path))\n",
    "        \n",
    "        shuffler = np.arange(len(train_images))\n",
    "        np.random.shuffle(shuffler)\n",
    "        train_images = train_images[shuffler]\n",
    "        train_annotations = train_annotations[shuffler]\n",
    "        \n",
    "        training_bp = Utils.BatchProcessor()\n",
    "        test_bp = Utils.BatchProcessor()\n",
    "        \n",
    "        train_scaling_vector = Utils.get_class_balance_vector(train_anno_path)\n",
    "        \n",
    "        for i in range(int(num_epochs*len(train_images) / batch_size)):\n",
    "            batch_X, batch_y = training_bp.get_next_batch(train_images, train_annotations, batch_size)\n",
    "\n",
    "            self.sess.run(self.train_step,feed_dict={self.images: batch_X,self.labels_im: batch_y, self.scaling_vector: train_scaling_vector})\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                accus = []\n",
    "                losss = []\n",
    "                for j in range(int(len(test_images)/batch_size)):\n",
    "                    test_batch_x, test_batch_y = test_bp.get_next_batch(test_images, test_annotations, batch_size)\n",
    "                    loss_o, accu_o = self.sess.run([self.loss,self.accuracy],feed_dict={self.images: test_batch_x,self.labels_im: test_batch_y, self.scaling_vector: train_scaling_vector})\n",
    "                    accus.append(accu_o)\n",
    "                    losss.append(loss_o)\n",
    "                train_summary = self.sess.run(train_summ,feed_dict={self.images: batch_X,self.labels_im: batch_y, self.scaling_vector: train_scaling_vector})\n",
    "                writer.add_summary(train_summary, i+10000)\n",
    "\n",
    "                test_sum = tf.Summary()\n",
    "                test_sum.value.add(tag=\"TestAccuracy\", simple_value =np.mean(accus))\n",
    "                writer.add_summary(test_sum, i+10000)\n",
    "                print(\"{2}/{3} Test accu: {0}, test loss: {1}\".format(np.mean(accus), np.mean(losss), i, int(num_epochs*len(train_images) / batch_size)))\n",
    "\n",
    "            if i % 1000 == 0 and i!=0:\n",
    "                if self.baysian:\n",
    "                    save_path = saver.save(self.sess, baysian_segnet_path)\n",
    "                else:\n",
    "                    save_path = saver.save(self.sess, segnet_path)\n",
    "                \n",
    "                print(\"Model saved to: %s\" % save_path)\n",
    "\n",
    "        \n",
    "    def encoding_layer(self, input, filter1, filter2, bias1, bias2, conv_stride, name=None, filter3=None, bias3=None, max_pool_stride = (1,2,2,1), dropout=False):\n",
    "        with tf.name_scope(name):\n",
    "            conv_1= tf.layers.batch_normalization(tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = input,filter=filter1, strides=conv_stride, padding=padding), bias1)))\n",
    "            conv_2= tf.layers.batch_normalization(tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = conv_1,filter=filter2, strides=conv_stride, padding=padding), bias2)))\n",
    "            if filter3 != None and bias3 != None:\n",
    "                conv_3 = tf.layers.batch_normalization(tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = conv_2,filter=filter3, strides=conv_stride, padding=padding), bias3)))\n",
    "                \n",
    "                max_pool, indicies = tf.nn.max_pool_with_argmax(conv_3, ksize=max_pooling_ksize,strides=(1,2,2,1), padding='VALID')\n",
    "            else:\n",
    "                max_pool, indicies = tf.nn.max_pool_with_argmax(conv_2, ksize=max_pooling_ksize,strides=(1,2,2,1), padding='VALID')\n",
    "            \n",
    "            if dropout:\n",
    "                return tf.nn.dropout(max_pool, keep_prob=1-dropout_prob), indicies\n",
    "            else:\n",
    "                return max_pool, indicies\n",
    "\n",
    "    def decoding_layer(self, input, indicies, filter1_shape, filter2_shape, name=None, filter3_shape=None, dropout=False):\n",
    "        with tf.name_scope(name):\n",
    "            upsample = Utils.unpool_with_argmax(input, indicies, name=name+\"Upsample\")\n",
    "            if filter3_shape != None:\n",
    "                deconv_1 = tf.layers.batch_normalization(tf.layers.conv2d(upsample, filter3_shape,(3,3),activation=tf.nn.relu, padding='same', kernel_initializer=tf.contrib.layers.variance_scaling_initializer()))\n",
    "                deconv_2 = tf.layers.batch_normalization(tf.layers.conv2d(deconv_1, filter2_shape,(3,3),activation=tf.nn.relu, padding='same',kernel_initializer=tf.contrib.layers.variance_scaling_initializer()))\n",
    "                max_pool =  tf.layers.batch_normalization(tf.layers.conv2d(deconv_2, filter1_shape,(3,3),activation=tf.nn.relu, padding='same', kernel_initializer=tf.contrib.layers.variance_scaling_initializer()))\n",
    "            else:\n",
    "                deconv_1 = tf.layers.batch_normalization(tf.layers.conv2d(upsample, filter2_shape,(3,3),activation=tf.nn.relu, padding='same',kernel_initializer=tf.contrib.layers.variance_scaling_initializer()))\n",
    "                max_pool =  tf.layers.batch_normalization(tf.layers.conv2d(deconv_1, filter1_shape,(3,3),activation=tf.nn.relu, padding='same', kernel_initializer=tf.contrib.layers.variance_scaling_initializer()))\n",
    "                \n",
    "            if dropout:\n",
    "                return tf.nn.dropout(max_pool, keep_prob=1-dropout_prob)\n",
    "            else:\n",
    "                return max_pool\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T20:15:06.378494Z",
     "start_time": "2017-12-10T20:12:59.466866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape: (3, 11, 15, 512), ind: (3, 11, 15, 512)\n",
      "input_shape: (3, 22, 30, 512), ind: (3, 22, 30, 512)\n",
      "input_shape: (3, 44, 60, 256), ind: (3, 44, 60, 256)\n",
      "input_shape: (3, 88, 120, 128), ind: (3, 88, 120, 128)\n",
      "input_shape: (3, 176, 240, 64), ind: (3, 176, 240, 64)\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[3,64,352,480]\n\t [[Node: Decoding_layer_5/conv2d/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Decoding_layer_5/Decoding_layer_5Upsample/Reshape_4, conv2d_11/kernel/read)]]\n\nCaused by op 'Decoding_layer_5/conv2d/Conv2D', defined at:\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-1cd6fcbfccf0>\", line 1, in <module>\n    segnet = SegNet(load_model=False, baysian=False)\n  File \"<ipython-input-2-9d4b74486aed>\", line 27, in __init__\n    self.build_model()\n  File \"<ipython-input-2-9d4b74486aed>\", line 113, in build_model\n    deconv_5 = self.decoding_layer(deconv_4, indicies_1, number_of_classes, w_1_2.shape[2].value, name= \"Decoding_layer_5\")\n  File \"<ipython-input-2-9d4b74486aed>\", line 214, in decoding_layer\n    deconv_1 = tf.layers.batch_normalization(tf.layers.conv2d(upsample, filter2_shape,(3,3),activation=tf.nn.relu, padding='same',kernel_initializer=tf.contrib.layers.variance_scaling_initializer()))\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\", line 608, in conv2d\n    return layer.apply(inputs)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 671, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 575, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\", line 167, in call\n    outputs = self._convolution_op(inputs, self.kernel)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 835, in __call__\n    return self.conv_op(inp, filter)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 499, in __call__\n    return self.call(inp, filter)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 187, in __call__\n    name=self.name)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 630, in conv2d\n    data_format=data_format, name=name)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[3,64,352,480]\n\t [[Node: Decoding_layer_5/conv2d/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Decoding_layer_5/Decoding_layer_5Upsample/Reshape_4, conv2d_11/kernel/read)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[3,64,352,480]\n\t [[Node: Decoding_layer_5/conv2d/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Decoding_layer_5/Decoding_layer_5Upsample/Reshape_4, conv2d_11/kernel/read)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1cd6fcbfccf0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msegnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSegNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbaysian\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-9d4b74486aed>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, load_model, baysian)\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-9d4b74486aed>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self, model_is_loaded)\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[0mbatch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_bp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_next_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_annotations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_im\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaling_vector\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_scaling_vector\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1336\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[3,64,352,480]\n\t [[Node: Decoding_layer_5/conv2d/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Decoding_layer_5/Decoding_layer_5Upsample/Reshape_4, conv2d_11/kernel/read)]]\n\nCaused by op 'Decoding_layer_5/conv2d/Conv2D', defined at:\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-1cd6fcbfccf0>\", line 1, in <module>\n    segnet = SegNet(load_model=False, baysian=False)\n  File \"<ipython-input-2-9d4b74486aed>\", line 27, in __init__\n    self.build_model()\n  File \"<ipython-input-2-9d4b74486aed>\", line 113, in build_model\n    deconv_5 = self.decoding_layer(deconv_4, indicies_1, number_of_classes, w_1_2.shape[2].value, name= \"Decoding_layer_5\")\n  File \"<ipython-input-2-9d4b74486aed>\", line 214, in decoding_layer\n    deconv_1 = tf.layers.batch_normalization(tf.layers.conv2d(upsample, filter2_shape,(3,3),activation=tf.nn.relu, padding='same',kernel_initializer=tf.contrib.layers.variance_scaling_initializer()))\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\", line 608, in conv2d\n    return layer.apply(inputs)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 671, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 575, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\", line 167, in call\n    outputs = self._convolution_op(inputs, self.kernel)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 835, in __call__\n    return self.conv_op(inp, filter)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 499, in __call__\n    return self.call(inp, filter)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 187, in __call__\n    name=self.name)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 630, in conv2d\n    data_format=data_format, name=name)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[3,64,352,480]\n\t [[Node: Decoding_layer_5/conv2d/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Decoding_layer_5/Decoding_layer_5Upsample/Reshape_4, conv2d_11/kernel/read)]]\n"
     ]
    }
   ],
   "source": [
    "segnet = SegNet(load_model=False, baysian=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T20:00:38.552842Z",
     "start_time": "2017-12-10T20:00:36.497223Z"
    }
   },
   "outputs": [],
   "source": [
    "images = Utils.load_images(test_data_path, amount=batch_size)\n",
    "prediciton = segnet.predict(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T18:01:32.132751Z",
     "start_time": "2017-12-10T18:01:31.922708Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T20:00:29.155868Z",
     "start_time": "2017-12-10T18:45:27.704272Z"
    }
   },
   "outputs": [],
   "source": [
    "segnet.train_model(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T20:00:57.616186Z",
     "start_time": "2017-12-10T20:00:57.602665Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(prediciton[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(prediciton[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T13:41:09.366077Z",
     "start_time": "2017-12-10T13:40:52.124631Z"
    }
   },
   "outputs": [],
   "source": [
    "annos = np.argmax(Utils.load_annotations(test_anno_path), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-10T13:41:15.132145Z",
     "start_time": "2017-12-10T13:41:14.923612Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(annos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-09T15:24:23.936001Z",
     "start_time": "2017-12-09T13:00:04.794420Z"
    }
   },
   "outputs": [],
   "source": [
    "segnet = SegNet(load_model=False, baysian=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
