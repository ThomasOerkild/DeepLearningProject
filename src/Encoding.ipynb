{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import Utils\n",
    "import keras\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg16 = keras.applications.VGG16(include_top=False,weights='imagenet')\n",
    "vgg16_graph = keras.backend.get_session().graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_1_1 = vgg16_graph.get_tensor_by_name('block1_conv1/kernel:0')\n",
    "b_1_1 = vgg16_graph.get_tensor_by_name('block1_conv1/bias:0')\n",
    "w_1_2 = vgg16_graph.get_tensor_by_name('block1_conv2/kernel:0')\n",
    "b_1_2 = vgg16_graph.get_tensor_by_name('block1_conv2/bias:0')\n",
    "\n",
    "w_2_1 = vgg16_graph.get_tensor_by_name('block2_conv1/kernel:0')\n",
    "b_2_1 = vgg16_graph.get_tensor_by_name('block2_conv1/bias:0')\n",
    "w_2_2 = vgg16_graph.get_tensor_by_name('block2_conv2/kernel:0')\n",
    "b_2_2 = vgg16_graph.get_tensor_by_name('block2_conv2/bias:0')\n",
    "\n",
    "w_3_1 = vgg16_graph.get_tensor_by_name('block3_conv1/kernel:0')\n",
    "b_3_1 = vgg16_graph.get_tensor_by_name('block3_conv1/bias:0')\n",
    "w_3_2 = vgg16_graph.get_tensor_by_name('block3_conv2/kernel:0')\n",
    "b_3_2 = vgg16_graph.get_tensor_by_name('block3_conv2/bias:0')\n",
    "w_3_3 = vgg16_graph.get_tensor_by_name('block3_conv3/kernel:0')\n",
    "b_3_3 = vgg16_graph.get_tensor_by_name('block3_conv3/bias:0')\n",
    "\n",
    "w_4_1 = vgg16_graph.get_tensor_by_name('block4_conv1/kernel:0')\n",
    "b_4_1 = vgg16_graph.get_tensor_by_name('block4_conv1/bias:0')\n",
    "w_4_2 = vgg16_graph.get_tensor_by_name('block4_conv2/kernel:0')\n",
    "b_4_2 = vgg16_graph.get_tensor_by_name('block4_conv2/bias:0')\n",
    "w_4_3 = vgg16_graph.get_tensor_by_name('block4_conv3/kernel:0')\n",
    "b_4_3 = vgg16_graph.get_tensor_by_name('block4_conv3/bias:0')\n",
    "\n",
    "w_5_1 = vgg16_graph.get_tensor_by_name('block5_conv1/kernel:0')\n",
    "b_5_1 = vgg16_graph.get_tensor_by_name('block5_conv1/bias:0')\n",
    "w_5_2 = vgg16_graph.get_tensor_by_name('block5_conv2/kernel:0')\n",
    "b_5_2 = vgg16_graph.get_tensor_by_name('block5_conv2/bias:0')\n",
    "w_5_3 = vgg16_graph.get_tensor_by_name('block5_conv3/kernel:0')\n",
    "b_5_3 = vgg16_graph.get_tensor_by_name('block5_conv3/bias:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpool_with_argmax(pool, ind, name = None, ksize=[1, 2, 2, 1]):\n",
    "\n",
    "    \"\"\"\n",
    "       Unpooling layer after max_pool_with_argmax.\n",
    "       Args:\n",
    "           pool:   max pooled output tensor\n",
    "           ind:      argmax indices\n",
    "           ksize:     ksize is the same as for the pool\n",
    "       Return:\n",
    "           unpool:    unpooling tensor\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        input_shape = pool.get_shape().as_list()\n",
    "        print(\"input_shape: {0}, ind: {1}\".format(pool.shape, ind.shape))\n",
    "        output_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])\n",
    "\n",
    "        flat_input_size = tf.reduce_prod(input_shape)\n",
    "        flat_output_shape = [output_shape[0], output_shape[1] * output_shape[2] * output_shape[3]]\n",
    "\n",
    "        pool_ = tf.reshape(pool, [flat_input_size])\n",
    "        batch_range = tf.reshape(tf.range(output_shape[0], dtype=ind.dtype), shape=[input_shape[0], 1, 1, 1])\n",
    "        b = tf.ones_like(ind) * batch_range\n",
    "        b = tf.reshape(b, [flat_input_size, 1])\n",
    "        ind_ = tf.reshape(ind, [flat_input_size, 1])\n",
    "        ind_ = tf.concat([b, ind_], 1)\n",
    "\n",
    "        ret = tf.scatter_nd(ind_, pool_, shape=flat_output_shape)\n",
    "        ret = tf.reshape(ret, output_shape)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_pooling_5: (4, 11, 15, 512)\n",
      "input_shape: (4, 11, 15, 512), ind: (4, 11, 15, 512)\n",
      "upsample_1: (4, 22, 30, 512)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable deconv_1_1/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-458a5e76a72e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mupsample_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpool_with_argmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_pooling_5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindicies_5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'upsample_1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"upsample_1: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupsample_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mdeconv_1_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupsample_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw_5_3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'deconv_1_1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"deconv_1_1: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeconv_1_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mdeconv_1_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeconv_1_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_5_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'deconv_1_2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(inputs, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[0;32m    606\u001b[0m       \u001b[0m_reuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m       _scope=name)\n\u001b[1;32m--> 608\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    669\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m     \"\"\"\n\u001b[1;32m--> 671\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m   def _add_inbound_node(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    557\u001b[0m           \u001b[0minput_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 559\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    560\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    141\u001b[0m                                     \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                                     \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m                                     dtype=self.dtype)\n\u001b[0m\u001b[0;32m    144\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m       self.bias = self.add_variable(name='bias',\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36madd_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[0;32m    456\u001b[0m                                    \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m                                    \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m                                    trainable=trainable and self.trainable)\n\u001b[0m\u001b[0;32m    459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvariable\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexisting_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mvariable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1201\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m       constraint=constraint)\n\u001b[0m\u001b[0;32m   1204\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m   1205\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1090\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1092\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m   1093\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1094\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    740\u001b[0m                          \u001b[1;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 742\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    743\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable deconv_1_1/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "conv_1_1 = tf.layers.batch_normalization(tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = images,filter=w_1_1, strides=strides, padding=padding), b_1_1)))\n",
    "conv_1_2 = tf.layers.batch_normalization(tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = conv_1_1,filter=w_1_2, strides=strides, padding=padding), b_1_2)))\n",
    "max_pooling_1, indicies_1 = tf.nn.max_pool_with_argmax(conv_1_2, ksize=max_pooling_ksize,strides=(1,2,2,1), padding='VALID')\n",
    "\n",
    "conv_2_1 = tf.layers.batch_normalization(tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = max_pooling_1,filter=w_2_1, strides=strides, padding=padding), b_2_1)))\n",
    "conv_2_2 = tf.layers.batch_normalization(tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = conv_2_1,filter=w_2_2, strides=strides, padding=padding), b_2_2)))\n",
    "max_pooling_2, indicies_2 = tf.nn.max_pool_with_argmax(conv_2_2, ksize=max_pooling_ksize,strides=(1,2,2,1), padding='VALID')\n",
    "\n",
    "conv_3_1 = tf.layers.batch_normalization(tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = max_pooling_2,filter=w_3_1, strides=strides, padding=padding), b_3_1)))\n",
    "conv_3_2 = tf.layers.batch_normalization(tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = conv_3_1,filter=w_3_2, strides=strides, padding=padding), b_3_2)))\n",
    "conv_3_3 = tf.layers.batch_normalization(tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = conv_3_2,filter=w_3_3, strides=strides, padding=padding), b_3_3)))\n",
    "max_pooling_3, indicies_3 = tf.nn.max_pool_with_argmax(conv_3_3, ksize=max_pooling_ksize,strides=(1,2,2,1), padding='VALID')\n",
    "\n",
    "conv_4_1 = tf.layers.batch_normalization(tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = max_pooling_3,filter=w_4_1, strides=strides, padding=padding), b_4_1)))\n",
    "conv_4_2 = tf.layers.batch_normalization(tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = conv_4_1,filter=w_4_2, strides=strides, padding=padding), b_4_2)))\n",
    "conv_4_3 = tf.layers.batch_normalization(tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = conv_4_2,filter=w_4_3, strides=strides, padding=padding), b_4_3)))\n",
    "max_pooling_4, indicies_4 = tf.nn.max_pool_with_argmax(conv_4_3, ksize=max_pooling_ksize,strides=(1,2,2,1), padding='VALID')\n",
    "\n",
    "conv_5_1 = tf.layers.batch_normalization(tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = max_pooling_4,filter=w_5_1, strides=strides, padding=padding), b_5_1)))\n",
    "conv_5_2 = tf.layers.batch_normalization(tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = conv_5_1,filter=w_5_2, strides=strides, padding=padding), b_5_2)))\n",
    "conv_5_3 = tf.layers.batch_normalization(tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = conv_5_2,filter=w_5_3, strides=strides, padding=padding), b_5_3)))\n",
    "#max_pooling_5, indicies_5 = tf.nn.max_pool_with_argmax(conv_5_3, ksize=max_pooling_ksize,strides=(1,2,2,1), padding='VALID', Targmax=tf.int32)\n",
    "max_pooling_5, indicies_5 = tf.nn.max_pool_with_argmax(conv_5_3, ksize=max_pooling_ksize,strides=(1,2,2,1), padding='VALID')\n",
    "\n",
    "print(\"max_pooling_5: {0}\".format(max_pooling_5.shape))\n",
    "upsample_1 = unpool_with_argmax(max_pooling_5, indicies_5, name='upsample_1')\n",
    "print(\"upsample_1: {0}\".format(upsample_1.shape))\n",
    "deconv_1_1 = tf.layers.batch_normalization(tf.layers.conv2d(inputs=upsample_1, filters=w_5_3.shape[2].value,kernel_size=(3,3),activation=tf.nn.relu, name='deconv_1_1',padding='same'))\n",
    "print(\"deconv_1_1: {0}\".format(deconv_1_1.shape))\n",
    "deconv_1_2 = tf.layers.batch_normalization(tf.layers.conv2d(deconv_1_1, w_5_2.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_1_2',padding='same'))\n",
    "print(\"deconv_1_2: {0}\".format(deconv_1_2.shape))\n",
    "deconv_1_3 = tf.layers.batch_normalization(tf.layers.conv2d(deconv_1_2, w_5_1.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_1_3',padding='same'))\n",
    "print(\"deconv_1_3: {0}\".format(deconv_1_3.shape))\n",
    "\n",
    "upsample_2 = unpool_with_argmax(deconv_1_3, indicies_4, name='upsample_2')\n",
    "deconv_2_1 = tf.layers.batch_normalization(tf.layers.conv2d(upsample_2, w_4_3.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_2_1',padding='same'))\n",
    "deconv_2_2 = tf.layers.batch_normalization(tf.layers.conv2d(deconv_2_1, w_4_2.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_2_2',padding='same'))\n",
    "deconv_2_3 = tf.layers.batch_normalization(tf.layers.conv2d(deconv_2_2, w_4_1.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_2_3',padding='same'))\n",
    "\n",
    "upsample_3 = unpool_with_argmax(deconv_2_3, indicies_3, name='upsample_3')\n",
    "deconv_3_1 = tf.layers.batch_normalization(tf.layers.conv2d(upsample_3, w_3_3.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_3_1',padding='same'))\n",
    "deconv_3_2 = tf.layers.batch_normalization(tf.layers.conv2d(deconv_3_1, w_3_2.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_3_2',padding='same'))\n",
    "deconv_3_3 = tf.layers.batch_normalization(tf.layers.conv2d(deconv_3_2, w_3_1.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_3_3',padding='same'))\n",
    "\n",
    "upsample_4 = unpool_with_argmax(deconv_3_3, indicies_2, name='upsample_4')\n",
    "deconv_4_1 = tf.layers.batch_normalization(tf.layers.conv2d(upsample_4, w_2_2.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_4_1',padding='same'))\n",
    "deconv_4_2 = tf.layers.batch_normalization(tf.layers.conv2d(deconv_4_1, w_2_1.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_4_2',padding='same'))\n",
    "\n",
    "upsample_5 = unpool_with_argmax(deconv_4_2, indicies_1, name='upsample_5')\n",
    "deconv_5_1 = tf.layers.batch_normalization(tf.layers.conv2d(upsample_5, w_1_2.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_5_1',padding='same'))\n",
    "#deconv_5_2 = tf.layers.conv2d(deconv_5_1, w_1_1.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_5_2',padding='same')\n",
    "deconv_5_2 = tf.layers.batch_normalization(tf.layers.conv2d(deconv_5_1,number_of_classes,(3,3),activation=tf.nn.relu, name='deconv_5_2',padding='same'))\n",
    "print(\"deconv_5_2: {0}\".format(deconv_5_2.shape))\n",
    "out = tf.nn.softmax(deconv_5_2)\n",
    "print(\"out: {0}\".format(out.shape))\n",
    "print(\"out: {0}\".format(labels_im))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=labels_im))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "correctPred = tf.equal(tf.argmax(out,-1), tf.argmax(labels_im,-1))\n",
    "print(\"correctPred: {0}\".format(correctPred.shape))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "print(\"acurracy: {0}\".format(accuracy.shape))\n",
    "\n",
    "#reverseOneHot = tf.argmax(out,-1)\n",
    "#reverseOneHot = tf.reshape(reverseOneHot,(reverseOneHot.shape[1].value,reverseOneHot.shape[2].value))\n",
    "#print(\"reverseOneHot: {0}\".format(reverseOneHot.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_of_classes = 12\n",
    "batch_size = 4\n",
    "input_shape = (batch_size, 352, 480, 3)\n",
    "label_shape = (batch_size, 352, 480, number_of_classes)\n",
    "\n",
    "images=tf.placeholder(dtype=tf.float32,shape=input_shape)\n",
    "labels_im=tf.placeholder(dtype=tf.float32,shape=label_shape)\n",
    "\n",
    "#images=tf.placeholder(dtype=tf.float32,shape=(1, 720, 960, 3))\n",
    "strides = (1,1,1,1)\n",
    "padding = 'SAME'\n",
    "max_pooling_ksize = (1,2,2,1)\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = np.asarray(Utils.load_images(\"../Data/CamVid/train/\"))\n",
    "test_images = np.asarray(Utils.load_images(\"../Data/CamVid/test/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_annotations = np.asarray(Utils.load_annotations(\"../Data/CamVid/trainannot/\"))\n",
    "test_annotations = np.asarray(Utils.load_annotations(\"../Data/CamVid/testannot/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_bp = Utils.BatchProcessor()\n",
    "test_bp = Utils.BatchProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(int(num_epochs*len(train_images) / batch_size)):\n",
    "    batch_X, batch_y = training_bp.get_next_batch(train_images, train_annotations, batch_size)\n",
    "    \n",
    "    sess.run(train_step,feed_dict={images: batch_X,labels_im: batch_y})\n",
    "    \n",
    "    if(i%50 == 0):\n",
    "        accus = []\n",
    "        losss = []\n",
    "        for j in range(int(len(test_images)/batch_size)):\n",
    "            test_batch_x, test_batch_y = test_bp.get_next_batch(test_images, test_annotations, batch_size)\n",
    "            loss_o, accu_o = sess.run([loss,accuracy],feed_dict={images: test_batch_x,labels_im: test_batch_y})\n",
    "            accus.append(accu_o)\n",
    "            losss.append(loss_o)\n",
    "            \n",
    "        print(\"Test accu: {0}, test loss: {1}\".format(np.mean(accus), np.mean(losss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "sess.run(max_pooling_5,feed_dict={images: image})\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "loss_sum,acc_sum = sess.run([loss,accuracy],feed_dict={images: image,labels_im: label_image})\n",
    "print(\"without training:\")\n",
    "print(\"Loss:\",loss_sum)\n",
    "print(\"Accuracy:\",acc_sum)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(0,100):\n",
    "    sess.run(train_step,feed_dict={images: image,labels_im: label_image})\n",
    "    loss_sum,acc_sum = sess.run([loss,accuracy],feed_dict={images: image,labels_im: label_image})\n",
    "    if i % 10 == 0:\n",
    "        print(\"Loss:\",loss_sum)\n",
    "        print(\"Accuracy:\",acc_sum)\n",
    "        pass\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roh = sess.run(reverseOneHot,feed_dict={images: image,labels_im: label_image})\n",
    "\n",
    "plt.imshow(roh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im = imread('../Data/CamVid/trainannot/0001TP_006690.png')\n",
    "im = im[0:192, 0:256]\n",
    "plt.imshow(im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
