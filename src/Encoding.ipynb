{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg16 = tf.keras.applications.VGG16(include_top=False,weights='imagenet')\n",
    "vgg16_graph = tf.keras.backend.get_session().graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_1_1 = vgg16_graph.get_tensor_by_name('block1_conv1/kernel:0')\n",
    "b_1_1 = vgg16_graph.get_tensor_by_name('block1_conv1/bias:0')\n",
    "w_1_2 = vgg16_graph.get_tensor_by_name('block1_conv2/kernel:0')\n",
    "b_1_2 = vgg16_graph.get_tensor_by_name('block1_conv2/bias:0')\n",
    "\n",
    "w_2_1 = vgg16_graph.get_tensor_by_name('block2_conv1/kernel:0')\n",
    "b_2_1 = vgg16_graph.get_tensor_by_name('block2_conv1/bias:0')\n",
    "w_2_2 = vgg16_graph.get_tensor_by_name('block2_conv2/kernel:0')\n",
    "b_2_2 = vgg16_graph.get_tensor_by_name('block2_conv2/bias:0')\n",
    "\n",
    "w_3_1 = vgg16_graph.get_tensor_by_name('block3_conv1/kernel:0')\n",
    "b_3_1 = vgg16_graph.get_tensor_by_name('block3_conv1/bias:0')\n",
    "w_3_2 = vgg16_graph.get_tensor_by_name('block3_conv2/kernel:0')\n",
    "b_3_2 = vgg16_graph.get_tensor_by_name('block3_conv2/bias:0')\n",
    "w_3_3 = vgg16_graph.get_tensor_by_name('block3_conv3/kernel:0')\n",
    "b_3_3 = vgg16_graph.get_tensor_by_name('block3_conv3/bias:0')\n",
    "\n",
    "w_4_1 = vgg16_graph.get_tensor_by_name('block4_conv1/kernel:0')\n",
    "b_4_1 = vgg16_graph.get_tensor_by_name('block4_conv1/bias:0')\n",
    "w_4_2 = vgg16_graph.get_tensor_by_name('block4_conv2/kernel:0')\n",
    "b_4_2 = vgg16_graph.get_tensor_by_name('block4_conv2/bias:0')\n",
    "w_4_3 = vgg16_graph.get_tensor_by_name('block4_conv3/kernel:0')\n",
    "b_4_3 = vgg16_graph.get_tensor_by_name('block4_conv3/bias:0')\n",
    "\n",
    "w_5_1 = vgg16_graph.get_tensor_by_name('block5_conv1/kernel:0')\n",
    "b_5_1 = vgg16_graph.get_tensor_by_name('block5_conv1/bias:0')\n",
    "w_5_2 = vgg16_graph.get_tensor_by_name('block5_conv2/kernel:0')\n",
    "b_5_2 = vgg16_graph.get_tensor_by_name('block5_conv2/bias:0')\n",
    "w_5_3 = vgg16_graph.get_tensor_by_name('block5_conv3/kernel:0')\n",
    "b_5_3 = vgg16_graph.get_tensor_by_name('block5_conv3/bias:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'block4_conv1/kernel:0' shape=(3, 3, 256, 512) dtype=float32_ref>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_4_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpool_with_argmax(pool, ind, name = None, ksize=[1, 2, 2, 1]):\n",
    "\n",
    "    \"\"\"\n",
    "       Unpooling layer after max_pool_with_argmax.\n",
    "       Args:\n",
    "           pool:   max pooled output tensor\n",
    "           ind:      argmax indices\n",
    "           ksize:     ksize is the same as for the pool\n",
    "       Return:\n",
    "           unpool:    unpooling tensor\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        input_shape = pool.get_shape().as_list()\n",
    "        print(\"input_shape: {0}, ind: {1}\".format(pool.shape, ind.shape))\n",
    "        output_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])\n",
    "\n",
    "        flat_input_size = tf.reduce_prod(input_shape)\n",
    "        flat_output_shape = [output_shape[0], output_shape[1] * output_shape[2] * output_shape[3]]\n",
    "\n",
    "        pool_ = tf.reshape(pool, [flat_input_size])\n",
    "        batch_range = tf.reshape(tf.range(output_shape[0], dtype=ind.dtype), shape=[input_shape[0], 1, 1, 1])\n",
    "        b = tf.ones_like(ind) * batch_range\n",
    "        b = tf.reshape(b, [flat_input_size, 1])\n",
    "        ind_ = tf.reshape(ind, [flat_input_size, 1])\n",
    "        ind_ = tf.concat([b, ind_], 1)\n",
    "\n",
    "        ret = tf.scatter_nd(ind_, pool_, shape=flat_output_shape)\n",
    "        ret = tf.reshape(ret, output_shape)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images=tf.placeholder(dtype=tf.float32,shape=(16, 256, 512, 3))\n",
    "#images=tf.placeholder(dtype=tf.float32,shape=(1, 720, 960, 3))\n",
    "strides = (1,1,1,1)\n",
    "padding = 'SAME'\n",
    "max_pooling_ksize = (1,2,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_pooling_5: (16, 8, 16, 512)\n",
      "input_shape: (16, 8, 16, 512), ind: (16, 8, 16, 512)\n",
      "upsample_1: (16, 16, 32, 512)\n",
      "deconv_1_1: (16, 16, 32, 512)\n",
      "deconv_1_2: (16, 16, 32, 512)\n",
      "deconv_1_3: (16, 16, 32, 512)\n",
      "input_shape: (16, 16, 32, 512), ind: (16, 16, 32, 512)\n",
      "input_shape: (16, 32, 64, 256), ind: (16, 32, 64, 256)\n",
      "input_shape: (16, 64, 128, 128), ind: (16, 64, 128, 128)\n",
      "input_shape: (16, 128, 256, 64), ind: (16, 128, 256, 64)\n"
     ]
    }
   ],
   "source": [
    "conv_1_1 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = images,filter=w_1_1, strides=strides, padding=padding), b_1_1))\n",
    "conv_1_2 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = conv_1_1,filter=w_1_2, strides=strides, padding=padding), b_1_2))\n",
    "max_pooling_1, indicies_1 = tf.nn.max_pool_with_argmax(conv_1_2, ksize=max_pooling_ksize,strides=(1,2,2,1), padding='VALID')\n",
    "\n",
    "conv_2_1 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = max_pooling_1,filter=w_2_1, strides=strides, padding=padding), b_2_1))\n",
    "conv_2_2 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = conv_2_1,filter=w_2_2, strides=strides, padding=padding), b_2_2))\n",
    "max_pooling_2, indicies_2 = tf.nn.max_pool_with_argmax(conv_2_2, ksize=max_pooling_ksize,strides=(1,2,2,1), padding='VALID')\n",
    "\n",
    "conv_3_1 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = max_pooling_2,filter=w_3_1, strides=strides, padding=padding), b_3_1))\n",
    "conv_3_2 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = conv_3_1,filter=w_3_2, strides=strides, padding=padding), b_3_2))\n",
    "conv_3_3 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = conv_3_2,filter=w_3_3, strides=strides, padding=padding), b_3_3))\n",
    "max_pooling_3, indicies_3 = tf.nn.max_pool_with_argmax(conv_3_3, ksize=max_pooling_ksize,strides=(1,2,2,1), padding='VALID')\n",
    "\n",
    "conv_4_1 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = max_pooling_3,filter=w_4_1, strides=strides, padding=padding), b_4_1))\n",
    "conv_4_2 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = conv_4_1,filter=w_4_2, strides=strides, padding=padding), b_4_2))\n",
    "conv_4_3 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = conv_4_2,filter=w_4_3, strides=strides, padding=padding), b_4_3))\n",
    "max_pooling_4, indicies_4 = tf.nn.max_pool_with_argmax(conv_4_3, ksize=max_pooling_ksize,strides=(1,2,2,1), padding='VALID')\n",
    "\n",
    "conv_5_1 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = max_pooling_4,filter=w_5_1, strides=strides, padding=padding), b_5_1))\n",
    "conv_5_2 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = conv_5_1,filter=w_5_2, strides=strides, padding=padding), b_5_2))\n",
    "conv_5_3 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input = conv_5_2,filter=w_5_3, strides=strides, padding=padding), b_5_3))\n",
    "#max_pooling_5, indicies_5 = tf.nn.max_pool_with_argmax(conv_5_3, ksize=max_pooling_ksize,strides=(1,2,2,1), padding='VALID', Targmax=tf.int32)\n",
    "max_pooling_5, indicies_5 = tf.nn.max_pool_with_argmax(conv_5_3, ksize=max_pooling_ksize,strides=(1,2,2,1), padding='VALID')\n",
    "\n",
    "print(\"max_pooling_5: {0}\".format(max_pooling_5.shape))\n",
    "upsample_1 = unpool_with_argmax(max_pooling_5, indicies_5, name='upsample_1')\n",
    "print(\"upsample_1: {0}\".format(upsample_1.shape))\n",
    "deconv_1_1 = tf.layers.conv2d(inputs=upsample_1, filters=w_5_3.shape[2].value,kernel_size=(3,3),activation=tf.nn.relu, name='deconv_1_1',padding='same')\n",
    "print(\"deconv_1_1: {0}\".format(deconv_1_1.shape))\n",
    "deconv_1_2 = tf.layers.conv2d(deconv_1_1, w_5_2.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_1_2',padding='same')\n",
    "print(\"deconv_1_2: {0}\".format(deconv_1_2.shape))\n",
    "deconv_1_3 = tf.layers.conv2d(deconv_1_2, w_5_1.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_1_3',padding='same')\n",
    "print(\"deconv_1_3: {0}\".format(deconv_1_3.shape))\n",
    "\n",
    "upsample_2 = unpool_with_argmax(deconv_1_3, indicies_4, name='upsample_2')\n",
    "deconv_2_1 = tf.layers.conv2d(upsample_2, w_4_3.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_2_1',padding='same')\n",
    "deconv_2_2 = tf.layers.conv2d(deconv_2_1, w_4_2.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_2_2',padding='same')\n",
    "deconv_2_3 = tf.layers.conv2d(deconv_2_2, w_4_1.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_2_3',padding='same')\n",
    "\n",
    "upsample_3 = unpool_with_argmax(deconv_2_3, indicies_3, name='upsample_3')\n",
    "deconv_3_1 = tf.layers.conv2d(upsample_3, w_3_3.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_3_1',padding='same')\n",
    "deconv_3_2 = tf.layers.conv2d(deconv_3_1, w_3_2.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_3_2',padding='same')\n",
    "deconv_3_3 = tf.layers.conv2d(deconv_3_2, w_3_1.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_3_3',padding='same')\n",
    "\n",
    "upsample_4 = unpool_with_argmax(deconv_3_3, indicies_2, name='upsample_4')\n",
    "deconv_4_1 = tf.layers.conv2d(upsample_4, w_2_2.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_4_1',padding='same')\n",
    "deconv_4_2 = tf.layers.conv2d(deconv_4_1, w_2_1.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_4_2',padding='same')\n",
    "\n",
    "upsample_5 = unpool_with_argmax(deconv_4_2, indicies_1, name='upsample_5')\n",
    "deconv_5_1 = tf.layers.conv2d(upsample_5, w_1_2.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_5_1',padding='same')\n",
    "deconv_5_2 = tf.layers.conv2d(deconv_5_1, w_1_1.shape[2].value,(3,3),activation=tf.nn.relu, name='deconv_5_2',padding='same')\n",
    "\n",
    "out = tf.nn.softmax(deconv_5_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 64, 128, 256)\n",
      "(16, 32, 64, 512)\n"
     ]
    }
   ],
   "source": [
    "print(conv_3_3.shape)\n",
    "print(conv_4_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = imread('../Data/Raw_images/0001TP_006690.png')\n",
    "#image = image#[0:360,0:480]\n",
    "image = image[0:256,0:512] \n",
    "image = np.reshape(image,(1,np.shape(image)[0],np.shape(image)[1],3))\n",
    "type(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 256, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "image_paths = glob.glob(\"../Data/Raw_images/*.png\")\n",
    "image_list = []\n",
    "for i in range(0,16):\n",
    "    image_list.append(imread(image_paths[i])[0:256,0:512])\n",
    "\n",
    "images_arr = np.asarray(image_list)\n",
    "print(images_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.6623270511627197 seconds ---\n",
      "--- 10.924550533294678 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "sess.run(max_pooling_5,feed_dict={images: images_arr})\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "sess.run(out,feed_dict={images: images_arr})\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
